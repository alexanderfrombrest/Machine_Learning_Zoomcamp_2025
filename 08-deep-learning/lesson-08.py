# -*- coding: utf-8 -*-
"""ML_zoomcamp_lesson_08.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fQnaixdGuXSFQ51aN2zGsl9Wb19Mi54m
"""

!wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip
!unzip data.zip

!pwd

! ls /content/data/train

import numpy as np
import torch

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""Model
For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.

You need to develop the model with following structure:

* The shape for input should be (3, 200, 200) (channels first format in PyTorch)
  * Next, create a convolutional layer (nn.Conv2d):
Use 32 filters (output channels)
  * Kernel size should be (3, 3) (that's the size of the filter)
  * Use 'relu' as activation
* Reduce the size of the feature map with max pooling (nn.MaxPool2d)
* Set the pooling size to (2, 2)
* Turn the multi-dimensional result into vectors using flatten or view
* Next, add a nn.Linear layer with 64 neurons and 'relu' activation
* Finally, create the nn.Linear layer with 1 neuron - this will be the output

The output layer should have an activation - use the appropriate activation for the binary classification case
As optimizer use torch.optim.SGD with the following parameters:

* torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms

# Define relevant variables for the ML task
batch_size = 20
num_classes = 2
learning_rate = 0.001
num_epochs = 20

# Device will determine whether to run the training on GPU or CPU.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 2. Define Image Transformations
# We need to ensure all images are the same size and converted to Tensors.
data_transforms = transforms.Compose([
    transforms.Resize((200, 200)),  # Resize: CNNs require fixed input dimensions (e.g., 224x224 for ResNet)
    transforms.ToTensor(),          # Convert image to PyTorch Tensor (0-1 range)
    transforms.Normalize(           # Normalize: Helps the neural network converge faster
        mean=[0.485, 0.456, 0.406], # Standard means for ImageNet
        std=[0.229, 0.224, 0.225]),   # Standard stds for ImageNet
    transforms.RandomRotation(50),
    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),
    transforms.RandomHorizontalFlip()

])

# 3. Load the data
data_train = '/content/data/train'
data_test = '/content/data/test'

train_full_dataset = torchvision.datasets.ImageFolder(root=data_train, transform=data_transforms)
test_dataset = torchvision.datasets.ImageFolder(root=data_test, transform=data_transforms)

# 4. Split the training data into training and validation sets
# We'll use 80% for training and 20% for validation
train_size = int(0.8 * len(train_full_dataset))
validation_size = len(train_full_dataset) - train_size

train_dataset, validation_dataset = torch.utils.data.random_split(train_full_dataset, [train_size, validation_size], generator=torch.Generator().manual_seed(SEED))

# 5. Create DataLoaders
# DataLoaders handle batching and shuffling efficiently during training
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

total_size = len(train_full_dataset) + len(test_dataset)
total_size

import matplotlib.pyplot as plt
def imshow(img):
    # Un-normalize for visualization
    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Get a batch of training data
dataiter = iter(train_loader)
images, labels = next(dataiter)

print(f"Showing one batch of images. Labels: {labels}")
imshow(torchvision.utils.make_grid(images))

# Create CNN
class hairCNN(nn.Module):
#  Determine what layers and their order in CNN object
  def __init__(self, num_classes):
     super(hairCNN, self).__init__()

     # FEATURE EXTRACTION
     # Input layer
     self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)
     self.relu = nn.ReLU()

     # Resize (pooling)
     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

     # CLASSIFICATION
     # Hidden layer
     self.fc1 = nn.Linear(in_features=313632, out_features=64)

     # Output Linear Layer
     self.fc2 = nn.Linear(in_features=64, out_features=1)

     # 5. Output Activation
     # "Use the appropriate activation for the binary classification case"
     # Sigmoid squashes output between 0 and 1 (Probability)
     # If we use nn.BCEWithLogitsLoss(), we dont need self.sigmoid here

  # Progresses data across layers
  def forward(self, x):
      # --- Feature Extraction ---
      x = self.conv1(x)
      x = self.relu(x)
      x = self.pool(x)

      # --- Flattening ---
      # "Turn the multi-dimensional result into vectors"
      x = x.view(x.size(0), -1)

      # --- Classification ---
      x = self.fc1(x)
      x = self.relu(x) # ReLU for the hidden layer

      x = self.fc2(x)
      return x

# --- STEP 1: SETUP (Do this ONCE) ---
model = hairCNN(num_classes)

# A. Define the Optimizer (The "Fixer")
# It links to model.parameters() so it knows WHAT to fix.
optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)

# B. Define the Loss Function (The "Scorecard")
# We use BCELoss because we have Sigmoid at the end
criterion = nn.BCEWithLogitsLoss()

from torchsummary import summary

# Move the model to the specified device
model.to(device)

# Call summary, ensuring the input tensor is generated on the correct device
summary(model, input_size=(3, 200, 200), device=str(device))

num_epochs = 10
history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy
        predicted = (torch.sigmoid(outputs) > 0.5).float()
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    epoch_loss = running_loss / len(train_dataset)
    epoch_acc = correct_train / total_train
    history['loss'].append(epoch_loss)
    history['acc'].append(epoch_acc)

    model.eval()
    val_running_loss = 0.0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for images, labels in validation_loader:
            images, labels = images.to(device), labels.to(device)
            labels = labels.float().unsqueeze(1)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_running_loss += loss.item() * images.size(0)
            predicted = (torch.sigmoid(outputs) > 0.5).float()
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()

    val_epoch_loss = val_running_loss / len(validation_dataset)
    val_epoch_acc = correct_val / total_val
    history['val_loss'].append(val_epoch_loss)
    history['val_acc'].append(val_epoch_acc)

    print(f"Epoch {epoch+1}/{num_epochs}, "
          f"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, "
          f"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}")

import pandas as pd
df = pd.DataFrame(history)

# What is the median of training accuracy for all the epochs for this model?
median_training_accuracy = df.acc.median()
median_training_accuracy

# What is the standard deviation of training loss for all the epochs for this model?
std_dv_training_loss = df.loss.std()
std_dv_training_loss

num_epochs = 10
history = {'acc': [], 'loss': [], 'test_acc': [], 'test_loss': []}

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy
        predicted = (torch.sigmoid(outputs) > 0.5).float()
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    epoch_loss = running_loss / len(train_dataset)
    epoch_acc = correct_train / total_train
    history['loss'].append(epoch_loss)
    history['acc'].append(epoch_acc)

    model.eval()
    test_running_loss = 0.0
    correct_test = 0
    total_test = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            labels = labels.float().unsqueeze(1)

            outputs = model(images)
            loss = criterion(outputs, labels)

            test_running_loss += loss.item() * images.size(0)
            predicted = (torch.sigmoid(outputs) > 0.5).float()
            total_test += labels.size(0)
            correct_test += (predicted == labels).sum().item()

    test_epoch_loss = test_running_loss / len(test_dataset)
    test_epoch_acc = correct_test / total_test
    history['test_loss'].append(test_epoch_loss)
    history['test_acc'].append(test_epoch_acc)

    print(f"Epoch {epoch+1}/{num_epochs}, "
          f"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, "
          f"Test Loss: {test_epoch_loss:.4f}, Test Acc: {test_epoch_acc:.4f}")

last_5_epochs_acc = history['test_acc'][5:]
mean_val_acc = sum(last_5_epochs_acc) / len(last_5_epochs_acc)
print(mean_val_acc)
